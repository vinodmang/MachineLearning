---
title: "Prediction with Machine Learning"
author: "Vinod M"
date: "Monday, May 18, 2015"
output: html_document
---

###Executive Summary

This study analyses data generated by sensors attached to a human body while performing a universal bicep curl both correctly and incorrectly and trains an algorithm to differentiate between them. It then uses the algorithm to predict the outcome(labelled "classe") in the dataset. The data for this study comes from another original study ,details here : http://groupware.les.inf.puc-rio.br/har.

###Exploratory Analysis

When we open the dataset in R we notice that a lot of columns have missing data except when the new_window parameter is set to "yes". Going through the original study document the researches have performed aggregates of raw data at specific intervals and these are not relevant to this study and hence those rows are excluded. WE then split the data into training and test sets.

```{r}
suppressWarnings(library(caret));
suppressWarnings(library(randomForest))

x <- read.csv("c:/RLearning/pml-training.csv",stringsAsFactors =FALSE)
x1 <- subset(x,new_window=="no")
intrain <- createDataPartition(x1$classe,p=.7,list=F)
trainx <- x1[intrain,]
testx <- x1[-intrain,]
```


Plotting some predictors show strong correlation among certain predictors. In the analysis below we will drop one of the strongly correlated columns.

```{r}
par(mfrow=c(1,2))
qplot(total_accel_belt,roll_belt,data=x1,color=user_name,geom=c("jitter"))
qplot(gyros_arm_x,gyros_arm_y,data=x1,color=user_name,geom=c("jitter"))

```


Next we drop the irrelevant columns (the first 7 colums). We also leave the classe for now and only extract the predictors

```{r}
cols = colnames(x)[8:159]

x2 <- trainx[,cols]
```



We check which columns have no data (zero or NA) 

```{r}
x2 <- apply(x2,2,as.numeric)
x2 <- data.frame(x2,stringsAsFactors =FALSE)
#find which columns have NA or zero values get these column names
sm <- apply(x2,2,sum,na.rm=TRUE)

```

We drop these columns as they do not contribute to the end result 

```{r}
x3 <- x2[,-which(sm ==0)]
```


We try to determine what set of predictors uniquely effect the outcome and this we obtain by doing a correlation among all predictors and determine which are highly correlated

```{r}
#function that calculates most highly correlated columns in order

correlationofpredictors <- function(inputdataframe,maxnum)
{
    # find the correlations
    cormatrix <- cor(inputdataframe)
    # set the correlations on the diagonal or lower triangle to zero,
    # so they will not be reported as the highest ones:
    diag(cormatrix) <- 0
    cormatrix[lower.tri(cormatrix)] <- 0
    # flatten the matrix into a dataframe for easy sorting
    fm <- as.data.frame(as.table(cormatrix))
    # assign human-friendly names
    names(fm) <- c("First.Variable", "Second.Variable","Correlation")
    # sort and print the top n correlations
    head(fm[order(abs(fm$Correlation),decreasing=T),],n=maxnum)
}

#find the top 25 highly
corr <- correlationofpredictors(x3,25)
#get the ones that correlate > 0.9 absoulte value
corvals <- corr[abs(corr$Correlation) > .9,]

print(corvals)
```

We drop one set of highly correlated columns and append the "classe" outcome for the subsequent training exercise.
```{r}
cls <- as.character(unique(corvals$First.Variable))
ins <- 0
#remove those columns from data
for(i in 1:length(cls)){ins[i] <- which(colnames(x3)==cls[i])}
x34 <- subset(x3,select = -ins)


classe <- trainx[,"classe"]

x4 <- cbind(x34,classe)
```

###Training and Prediction

We use the random forest algorithm to train the data. Cross Validation is performed using k=6 folds. 

```{r cache=TRUE}

tr <- trainControl(method = "cv", number = 6)
zz <- train(x4$classe ~ ., method = "rf", trControl = tr, 
    x4)
print(zz)
```
The insample error rate is at the matching mtry  = (1-.99)x100 = 1%.

We use the trained model to predict on the test data :

```{r}
zz1 <- predict(zz,newdata=testx)
errorrate <- 100 - sum(zz1 == testx$classe)*100/nrow(testx) 
```
The out-ofsample error rate is `r errorrate` %

Finally we apply this model to the test data set https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
```{r}
test <-  read.csv("c:/RLearning/pml-testing.csv",stringsAsFactors =FALSE)
zz2 <- predict(zz,newdata=test)
print(zz2)
```

Comparing to the actual test results shows this 100% accurate
